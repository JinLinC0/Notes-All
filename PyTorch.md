# `PyTorch`

## 基础语法

### 矩阵的创建

- 创建一个全0的矩阵：![image-20241017201755771](D:\Myproject\项目学习文档\images\image-20241017201755771.png)

  > 创建一个全0的5行3列的矩阵，`tensor`表示一个有张量，当作矩阵即可，`tensor`是`PyTorch`底层所支持的一个格式，所有的输入，所有的计算，都是对`tensor`所执行的
  >
  > `e-39`表示是一个很小的数，看成0即可

- 初始化一个全0的矩阵：![image-20241017202519602](D:\Myproject\项目学习文档\images\image-20241017202519602.png)

- 创建一个随机的矩阵：![image-20241017202054519](D:\Myproject\项目学习文档\images\image-20241017202054519.png)

  > 创建一个随机的5行3列的随机矩阵

  ![image-20241017203011686](D:\Myproject\项目学习文档\images\image-20241017203011686.png)

  > 创建一个随机的矩阵还可以使用这个方法：先创建一个5行3列的矩阵，在往矩阵中传入值

- 展示矩阵的大小：![image-20241017203153143](D:\Myproject\项目学习文档\images\image-20241017203153143.png)

  > 在跑机器学习算法的时候，建议执行完每一步操作后，打印一下矩阵的维度看看，方便对结果正确性的考量

### 数据的传入

直接传入值：![image-20241017202911530](D:\Myproject\项目学习文档\images\image-20241017202911530.png)

### 基本的计算方法

- 矩阵的加法：矩阵的加法有两种操作方式：

  ![image-20241017203420292](D:\Myproject\项目学习文档\images\image-20241017203420292.png)

- 索引：第一个：表示取所有，具体数值表示取具体哪一个，和`python`中的索引方式一致![image-20241017203520168](D:\Myproject\项目学习文档\images\image-20241017203520168.png)

- 改变矩阵的维度：

  ![image-20241017203739638](D:\Myproject\项目学习文档\images\image-20241017203739638.png)

  > `view(16)`表示将矩阵拉成一行的向量，16表示一行中有16个元素
  >
  > `view(-1, 8)`表示第二个维度有8个元素，-1表示根据原始的矩阵和另一个维度取进行自动的当前维度的计算

- 与`Numpy`进行协同操作：

  将`tensor`格式的数据转换成`numpy`中其他数据类型的格式，如一个数组：

  ![image-20241017204208769](D:\Myproject\项目学习文档\images\image-20241017204208769.png)

  > 通过`numpy`将`tensor`类型的数组进行格式的转换，转换为`numpy`所支持的格式，市后续可以进行交互的计算

  将`numpy`数据类型的格式转换成`tensor`格式的数据：

  ![image-20241017205705355](D:\Myproject\项目学习文档\images\image-20241017205705355.png)

  总之：`numpy`格式的数据可以和`tensor`格式的数据进行互相转换和交互



## 自动求导机制

前向传播理解起来比较容易，但是反向传播理解起来是比较困难的，在反向传播中，我们一般需要对每个参数进行求导（对矩阵w进行逐层的求导），我们对矩阵进行求导往往难度是比较大的，这就需要使用`PyTorch`中的自动求导机制，帮我们将反向传播全部计算好了，我们只需要将时间放在，怎么设计网络，怎么构建网络模型即可

![image-20241017210602890](D:\Myproject\项目学习文档\images\image-20241017210602890.png)

> 其中`requires_grad=True`表示可以对当前指定的矩阵进行求导操作，我们一般在构建这个矩阵的时候，直接将可以求导的这个参数进行传入

```py
b = torch.randn(3, 4, requires_grad=True)
t = x + b
# 对t矩阵进行求和，理解时可以将y当作一个损失函数
y = t.sum()
y
```

![image-20241017211044740](D:\Myproject\项目学习文档\images\image-20241017211044740.png)

```py
# 对y进行一个反向传播
y.backward()
```

对b进行一个求导操作：由于`t = x + b`，也可以直接看出b的导数都是1

![image-20241017211203637](D:\Myproject\项目学习文档\images\image-20241017211203637.png)

![image-20241017211311577](D:\Myproject\项目学习文档\images\image-20241017211311577.png)

> 当y在做一个反向传播的时候，凡是用到的那个参数，会自动的将所用到的参数的`requires_grad`属性设置为`True`

其他的例子：

![image-20241017211629779](D:\Myproject\项目学习文档\images\image-20241017211629779.png)

![image-20241017211907929](D:\Myproject\项目学习文档\images\image-20241017211907929.png)

![image-20241017212043568](D:\Myproject\项目学习文档\images\image-20241017212043568.png)

> `is_leaf`表示是否为子叶节点

![image-20241017212256315](D:\Myproject\项目学习文档\images\image-20241017212256315.png)

> `b.grad`每一次是1，上面因为执行的8次，所以变成了8