# 论文阅读笔记

## `Cross-Modal Learning for Anomaly Detection in Fused Magnesium Smelting Process: Methodology and Benchmark`

文章题目：Cross-Modal Learning for Anomaly Detection in Fused Magnesium Smelting Process: Methodology and Benchmark（熔融镁冶炼过程中异常检测的跨模态学习方法与基准测试）

***

关键词：Cross-modal learning（跨模态学习）, anomaly detection（异常检测）,
Transformer, fused magnesium furnace（熔镁炉）

***

研究背景：电熔镁炉的异常检测是保障其高效、稳定和安全运行的必要手段；但是现有的异常检测通常只是针对于单模态的异常检测，忽略了模态间信息的内在相关性

***

研究现状：

- 根据异常形成的机理，对单一的冶炼的时序电流异常进行检测，但是精确和可靠的检测是及其困难（电阻变化、液体翻滚造成的高频电流波动）
- 基于深度学习对电熔镁炉的图像和视频进行异常检测：由于电熔镁炉在异常状态下具有明显的视觉特征（局部区域红点），但是视觉干扰（遮挡）会影响异常检测的保真度

因此：综合利用视频和电流信息对电熔镁冶炼过程进行全面的异常检测至关重要

***

系统模型：`FmFormer`（跨模态（视频模态和时序电流数据模态）的`Transformer`）

***

文章贡献：

- 提出了新的多尺度标记化范式用于构建多尺度的令牌集：通过多尺度的方式，弥补`3D`视频模态和`1D`时序模态的之间的维度差距，将不同的多模态输入无缝地转换为具有等效维度的特征，使后续的注意力机制可以有效的探索多个模态中内在的特征；同时该多尺度标记样式有利于像素级的异常检测，通过多尺度标记从粗到细进行分层重组

  - 视频标记化的具体实现：

    在空间维度上稀疏的对输入视频中的元素进行采样，产生大小相同但是具有更大局部视野的补丁，局部视野通过膨胀率d进行控制，标准标记化（产生Pv补丁集）和扩张标记化（产生大小相同但是有更大局部视野的补丁集Pdv）的结合构造了多尺度视频补丁集，最后`3D`的补丁被展平为`1D`的向量，并映射到维度为1*D的令牌

    提出的多尺度标记化模块的公式为：

  - 时序数据标记化的具体实现：

    时序序列其本身就是一组较好的令牌，我们只需要将其映射到`Transformer`层所需维度的令牌

    ![22a040e2862cf97f3e22bf31cf8dbe0](..\images\22a040e2862cf97f3e22bf31cf8dbe0.jpg)

  多尺度标记化的作用：弥补不同维度模态的维度差距，将注意力机制优雅的迁移到跨模态的学习

- 提出了`FmFormer`模型架构，将多模态引入了电熔镁炉的异常检测领域，用于熔融镁冶炼过程中类级和像素级的异常检测，主要的实现方式是：交替堆叠的自我注意（编码每个模态的内部特征）和交叉注意的级联结构（编码跨模态的相关特征，对视频和时序电流模态的信息进行交互建模，探索两个模态之间的双向相关性）/（通过自注意力机制来学习不同模态中的内部特征；通过双向交叉注意力机制来捕捉不同模态间的相关性）

  ![image-20241023110628636](..\images\image-20241023110628636.png)

  > - 模型架构有两个输入源：`3D`的视频（2个空间维度和1个时间维度）；`1D`的三向电流数据（1个时间维度）
  > - 通过令牌化模块将`3D`视频和`1D`电流转换成两组`1D`特征
  > - 再将这些特征来通过跨模态`Transformer`编码器来探查两个模态之间的相关性，使用交替级联多层自我注意和交叉注意
  >
  >   - 首先需要进行端到端的训练，使两个输入令牌可以在嵌入空间中进行粗略的对齐
  >   - 使用多头注意力机制进行对输入的处理
  >   - 使用双向交叉注意力的相关特征编码，促进了不同模态的特征探索
  > - 最后使用多头解码器（由两部分组成：用于类级异常检测的分类头和用于发现异常区域的密集预测头）实现像素级异常检测
  >   - 密集预测头：将多尺度视频令牌创新组装成类似于图像特征的表示，逐步融合来发现异常区域
  >   - 分类头：将类别标记转换为异常预测，Transformer编码器使用两个小的MLP处理类标记，然后通过简单的线性融合机制获得检测结果

- 提出了一个先驱性的熔融镁冶炼过程的跨模态基准，同步采集超过220万个样本的视频和电流数据，确保正常和异常样本的比例接近1:1

***

文章结论：

`FmFormer`模型与几种先进的基于学习的异常检测方法进行比较，同时进行了单模态输入和交叉模态（跨模态）的比较

对于`FmFormer`模型不同输入模态的结果分析：

- 使用纯电流信息（使用电流信息的单模态）的异常检测离实际的工业应用还很远（性能较差）
- 单单使用视觉模态比使用当前电流信息有着更稳定和突出的视觉特征，性能也大幅度的提高，因为视觉特征比当前的电流特征更稳定
- 使用跨模态的方式进行异常检测，其性能相较于使用单视觉模态的性能更优，当重水雾遮挡了异常的视觉特征，还可以能依赖于来自前几帧和三相交流电的信息进行异常检测

`FmFormer`模型相比其他模型展现出了较好的性能，证实了其对类级预测和像素级预测的有效性，尽管使用单一电流模态时检测精度较差，但通过简单的线性融合可以提高模型的综合性能，通过跨模态学习的优势，在电流波动和强水雾遮挡等极端干扰下还可以实现准确的异常检测

***

文章亮点：

- 多模态异常检测：多模态的好处：当一个模态受到极端条件的干扰时，可以通过对另一个模态进行异常检测，提高了异常检测的准确率

- 多尺度标记规范：通过多尺度的方式，弥补`3D`视频模态和`1D`时序模态的之间的维度差距，将不同的多模态输入无缝地转换为具有等效维度的特征
- 结合`Transformer`提出了`FmFormer`模型架构，通过使用交替堆叠的自我注意和交叉注意的级联结构来挖掘模态内部的特征和捕捉不同模态间的相关性

***

自我想法：

看完这篇论文，我对多模态有了更加深刻的了解，结合多模态可以使工业中的异常检测性能超过使用任意一个单模态的性能，结合了`Transformer`中的自注意力机制和交叉注意力机制，可以和好的自身模态的特性和不同模态的相关性

- 如果依托这篇文章上做创新研究，我认为后续的创新点可以偏向特征提取或者算法优化

  - 特征提取：找数据集里面的一些数据特征，通过一些算法去数据挖掘，优化数据特征提取能力从而提高模型的性能和精度
  - 算法优化：不能套用现有的算法，需在文中算法的基础上进行优化改进，改进的算法需要在准确度和复杂度上保持优越性，或者适当牺牲一个指标，提高另一个指标

  但是文中多模态的方式对于算力要求较高，特别是处理视频模态的数据时

- 如果只研究时序电流数据，可以做回归任务的时序预测和分类任务的异常检测，或者先进行时序预测，再对预测的数据做一个异常检测

  - 结合一些比较先进的模型进行搭建算法，融入一些优秀的模块，在时序电流的数据集上做验证，在论文中可以看出使用单一的当前电流模态的`FmFormer`模型异常检测的精确度是比较低的，只有70%多，是否还有提高的空间？

  - 同时也可以将搭建好的算法在公开的数据集中做一个验证，判断改进的算法是否具有较强的普适性

  这个方向对于算法的要求较高



`Transformer`知识点补充：

`Transformer`能够利用分布式`GPU`进行并行训练，提升模型训练效率；自注意力机制可以使模型根据输入序列中的不同部分来赋予不同的注意权重，从而更好地捕捉语义关系；多头注意力可以使每个头学习不同的注意权重，从而捕捉不同类型之间的关系

架构：

![image-20240911202748220](..\images\image-20240911202748220.png)

- 输入部分：图中粉色的部分，最底部的两个部分（`inputs`：源文本；`Outputs`：目标文本）文本先进入文本的嵌入层：`Embedding`，再进入位置编码器的处理

- 输出部分：最顶部的部分（`Output Probabilites`）输入需要先经过线性层，再经过`softmax`层，最后输出

- 编码器部分：左边框中的内容是编码器的部分

  > 由`N`个编码器层堆叠而成的（程序员来指定有多少个编码器）；对于每个编码器层由两个子层连接结构组成；第一个子层（下面的）连接结构包括一个多头自注意力子层（`Multi-Head Attention`）和规范化层（`Add & Norm`）以及一个残差连接（底部的线往上连接（跨层向上传递我们的信息））；第二个子层连接结构包括一个前馈全连接子层（`Feed Forward`）和规范化层以及一个残差连接

- 解码器部分：右边框中的内容是解码器的部分

  > 由`N`个编码器层堆叠而成的（程序员来指定有多少个编码器）；对于每个编码器层由三个子层连接结构组成；第一个子层（最下面的）连接结构包括一个多头自注意力子层（加了一个掩码的）（`Masked Multi-Head Attention`）和规范化层（`Add & Norm`）以及一个残差连接（底部的线往上连接（跨层向上传递我们的信息））；第二个子层连接结构包括一个多头自注意力子层（`Multi-Head Attention`）和规范化层（`Add & Norm`）以及一个残差连接；第三个子层（最上面的）连接结构包括一个前馈全连接子层（`Feed Forward`）和规范化层以及一个残差连接；

编码器和解码器之间的连接是通过编码器的输出进入到解码器里面去的

***

自注意力的结构和QKV（Query查询向量、Keys键向量、Values值向量）的概念：

![image-20241024090426970](..\images\image-20241024090426970.png)

计算自注意力的步骤：

1. 从每个编码器的输入向量与对应的权重矩阵（WQ*,*WK*,*WV）相乘，生成QKV三个向量，新生成的向量的维度往往比输入的向量维度更低
2. 计算打分：对输入的部分进行打分，每个单词的输入都对应着一个q和k，将单词的查询向量和键向量（自己的键向量和其他单词输入的键向量）进行点击来计算分值，将分值除以8，使梯度更加稳定，再通过softmax传递结果，softmax的作用是使所有单词的分数归一化，得到的分数都是正值且和为1
3. 使用值向量乘以softmax分数，关注语义上相关的单词，弱化不相关的单词
4. 对权值向量进行求和，得到自注意力层在该位置的输出，之后将得到的向量传递给前馈神经网络

![image-20241024092448155](..\images\image-20241024092448155.png)

多头注意力机制：自注意力层的完善

- 扩展了模型专注于不同位置的能力
- 有多个查询/键/值权重矩阵集合，并且每一个都是随机初始化的
- 自注意力机制只是使用了一组WQ、WK、WV来进行变换得到查询、键、值矩阵，而多头注意力机制使用多组WQ，WK，WV得到多组查询、键、值矩阵，然后每组分别计算得到一个Z矩阵，最后将所有的Z矩阵拼接到一起乘以W0得到一个融合所有注意力头信息的矩阵Z，先经过了一步Add&Normalize，再传送到前馈神经网络

***

位置编码：

我们需要添加位置编码，如果不添加位置编码，那么无论单词在什么位置，它的注意力分数都是确定的，这是不合理的

***

Add&Normalize层：由 Add 和 Norm 两部分组成，计算公式为：

`LayerNorm(X + MiltiHeadAttention(X))`或者`LayerNorm(X + FeedForward(X))`

其中 X表示 Multi-Head Attention 或者 Feed Forward 的输入，MultiHeadAttention(X) 和 FeedForward(X) 表示输出

- Add：是在z的基础上加了一个残差块X，具体在公式中的表现为`X + MiltiHeadAttention(X)`

  加入残差块的目的是为了防止在深度神经网络的训练过程中发生退化的问题，退化的意思就是深度神经网络通过增加网络的层数，Loss逐渐减小，然后趋于稳定达到饱和，然后再继续增加网络层数，Loss反而增大

- Norm：归一化，归一化的目的是加快训练速度，提高训练的稳定性，在公式中的表现为`LayerNorm()`

***

全连接层`Feed Forward`，计算公式为：![image-20241024094732398](..\images\image-20241024094732398.png)

全连接层是一个两层的神经网络，先线性变换，然后ReLU非线性，再线性变换

这两层网络就是为了将输入的Z映射到更加高维的空间中然后通过非线性函数ReLU进行筛选，筛选完后再变回原来的维度



## `Wavelet Knowledge-Driven Transformer for Intelligent Machinery Fault Detection with Zero-Fault Samples`

文章题目：Wavelet Knowledge-Driven Transformer for Intelligent Machinery Fault Detection with Zero-Fault Samples（基于小波知识驱动的Transformer无故障样本智能故障检测）

***

关键词：Zero-fault Samples（无故障样本）, Continuous wavelet transform knowledgebase（连续小波变换知识库）, Contrastive loss functions（对比损失函数）, Fault detection（故障函数）

***

研究背景：为了克服实际故障样本不足的局限性，提出了一种不依赖于任何故障样本的故障检测方法

***

研究现状：

- 有些学者通过迁移学习的方法，充分利用从实验室中得到的故障样本，提高真实工业场景中故障检测的性能

  但是基于迁移学习的方法有一个严格的前提条件，需要目标数据和源数据分布之间具有一定程度的相似性

- 又有部分的学者通过数据生成技术，扩大了深度学习模型的训练样本，使用对抗网络自动生成假故障样本，但是机械监测信号的数据分布是不可预测的，生成的样本不包括这些变化特征

- 零触发学习的方法（`zeroshot`学习）得到了大量的研究

***

系统模型：本文提出了一种基于小波变换知识驱动的机械故障诊断方法：利用不同的小波母函数建立连续小波变换知识库（`CWTKB`），对监测信号进行多角度的分析，揭示实时监测信号的潜在状态特征。然后，**通过度量不同时频特征之间的相似性，构造了一种新的对比损失函数来优化特征编码器**，实现了无故障样本的特征编码器。最后，基于所提取的监测信号的代表性特征，开发了一种故障检测算法，用于检测机械设备的异常状态

![image-20241118203856523](..\images\image-20241118203856523.png)

- `CWTKB`被应用于从监测信号中获得不同的时频特征：与短时间傅里叶变化相比，小波变换结合了缩放窗口，有效的解决了时频域中分辨率不匹配的问题，信号利用`CWTKB`得到了不同的时频谱

- `ViT`被用作特征编码器来提取信号样本的代表性特征：`vit`作为特征编码器提供了上级特征提取的能力

- 对比损失函数是`ViT`训练的基础

  利用小波变换知识库对信号样本进行分析，得到信号的时频特征，基于这个假设，设计了一种新的对比损失函数

- `FD`是通过设计的训练和故障检测过程来实现的

  异常检测是根据两个连续样本之间存在显著水平的相似性假设来实现的

***

文章贡献：

1. 采用多个小波函数构造`CWTKB`，从多个时频角度对监测信号进行综合性分析，揭示实时监测信号的潜在状态特征
2. 设计了三种新的对比损失函数，实现了无故障样本的特征编码器训练
3. 充分利用了多种信号分析方法输出结果的侧重点差异和实时监测信号的强时序特性，能够有效地训练深度网络模型

***

文章亮点：故障检测摆脱了对故障样本的依赖，成功地克服了实际场景中无法获得真实故障样本的困境

***

自我想法：将时域信号转化到频域中进行特征的相似度比较是本篇文章的一个亮点，是后续研究时序数据可以进行借鉴的地方



## 有监督学习

有监督学习的主要目标是利用一组带标签的数据，学习从输入到输出的映射，然后将这种映射关系应用到未知数据上，达到分类或者回归的目的





## `ALIGNVSR: AUDIO-VISUAL CROSS-MODAL ALIGNMENT FOR VISUAL SPEECH RECOGNITION`

文章题目：ALIGNVSR: AUDIO-VISUAL CROSS-MODAL ALIGNMENT FOR VISUAL SPEECH RECOGNITION（ALIGNVSR：用于视觉语音识别的视听跨模态对齐）

文章贡献：

提出了`ALIGNVSR`模型用来对齐视频模态的数据和音频模态的数据（基于细粒度视听跨模态对齐的`VSR`方法，该方法使用音频和视频之间的时间对应关系来调节视频特征和一组音频单元之间的交叉注意力，其形式为额外的对准损失）

![image-20241121110434214](..\images\image-20241121110434214.png)

- 全局对齐：对准的实现从每个视频帧到音频单元组的交叉注意力过程，即视频特征是查询（Q），而音频单元作用键（K）和值（V），这种对齐是全局的对齐，视频可以与任何音频单元进行对齐（为了更好地保留视频的时间信息，在视频特征中涉及位置编码）
- 局部对齐：全局对齐不利用音频和视频模态之间的时间对齐，这是一个重要的信息源，因此设计了局部对齐机制来解决这个问题，u表示音频单元经量化的数目，v表示视频特征序列，其他T是视频帧的总数，a表示音频特征序列（由于采样频率不同，一个视频帧对应三个音频帧）

***

文章亮点：在模态对齐方面，提出了全局对齐和局部对齐两个对齐方式

开源的代码仓库：https://github.com/liu12366262626/AlignVSR.

***

自我想法：对于镁化炉的两个模态数据，是否可以引入本篇论文中的全局对齐，先进行一个初步的对齐，在结合clip思想进行进一步的对齐



## 基于标签的对比学习任务

### `CLMLF:A Contrastive Learning and Multi-Layer Fusion Method for Multimodal Sentiment Detection`

文章题目：CLMLF:A Contrastive Learning and Multi-Layer Fusion Method for
Multimodal Sentiment Detection（CLMLF：一种基于对比学习和多层融合的多模态情感检测方法）

***

研究背景：与单模态的数据相比，多模态数据可以提供更多的特征来帮助分析数据的情感

***

研究现状：在以往的工作中很少考虑标记级的特征融合，也很少有学者研究探索学习多模态数据中与情感相关的共同特征来帮助模型融合多模态特征

前人提出了基于变换编码器的多层融合（MLF）模块，利用`Transformer`中的多头自注意，可以捕捉数据向量的内在相关性，因此，具有显式和隐式关系的文本标记和图像块将彼此具有更高的注意力权重分配，这意味着MLF模块可以帮助更好地对齐和融合标记级文本和图像特征（MLF是一个多层编码器，它可以帮助提高模型的抽象能力，获得多模态数据中的深层特征）

对比学习在多模态领域中也被广泛的使用

***

文章贡献：在本文中，提出了一种对比学习和多层融合（CLMLF）方法，用于多模态情感检测

除了情感分析的任务，文章还设计了两个对比学习任务：基于标签的对比学习（`LBCL`）和基于数据的对比学习任务（`DBCL`），有助于模型学习共同的特征，大大提高模型的性能

本文主要的贡献在于：

- 提出了一种基于`Transformer-Encoder`的多层融合模型，利用多方向自注意的思想实现了文本和图像的标记级特征的对齐和融合，并利用MLF的深度提高了模型的抽象能力，MLF体系结构简单而有效

  ![image-20241119160843178](..\images\image-20241119160843178.png)

  上图展示了用于多模态情感检测的CLMLF模型的整体架构，该模型由两个模块组成：多层融合模块和多任务学习模块

  - 右侧是多层融合模块

    文本（BERT）-图像（ResNet）编码器（获得文本和图像的隐藏表示）

    `ResNet`编码器提取后的特征需要进行初步的处理，处理后的图像特征为：

    ![image-20241125101658581](..\images\image-20241125101658581.png)

    > `flatten`函数的功能是通过将前两个维度整形为一维来平坦化输入向量

    图像Transformer层

    提取完的文本和图像特征中需要将图像特征的维度转化为与文本特征相同的维度，第一次提取的图像特征输入到基于多层Transformer-Encoder的图像Transformer层，得到图像序列特征的最终编码（后续与文本的特征进行融合）

    ![image-20241125102734229](..\images\image-20241125102734229.png)

    > TEI表示图像的原始Transformer-Encoder

    文本-图像Transformer融合层模块（一个新的多层变换器编码器），利用多层融合模块对文本和图像的标记级特征进行对齐和融合

    将文本的特征与图像序列特征连接起来，再使用一个多层变换器编码器作为文本图像融合层，对齐和融合多模态特征，得到文本和图像的融合序列特征

    ![image-20241120100248113](..\images\image-20241120100248113.png)

    > TEM表示多模态数据的原始Transformer-Encoder

    在获得了文本和图像融合的序列特征后，序列特征不能用于分类任务，因此，我们使用一个简单的注意力层来获得多模态表示R

    ![image-20241125103020816](..\images\image-20241125103020816.png)

    > GELU是一个非线性函数

  - 左侧为多任务学习模块

    包括三个任务，情感分类，基于标签的对比学习和基于数据的对比学习任务

    - 基于标签的对比学习，主要利用了情感标签，为了让模型学习多模态数据中情感相关的特征

      根据其情感标签将每个批次中的数据分为正面和负面示例，对于多模态数据的负标签，批次中具有相同负标签的数据作为正例（粉红色的正方形），而没有负标签的数据作为负例（灰色的正方形）

      该算法主要包括两个步骤：第一步是根据批量中的数据标签生成去掩码标签；第二步是计算损失矩阵，利用去掩码标签和损失矩阵得到最终的损失，损失函数计算过程的伪代码如下所示：

      ![image-20241125131344222](..\images\image-20241125131344222.png)
    
      > 上述的伪代码主要用于基于标签的多模态对比学习任务，结合了文本T和图像I数据
      >
      > 1中Lc是一个包含三个子列表的列表，分别对应于三种情感类别：正向（0）、中性（1）和负向（2）；Lt是一个空列表，用于存储每个情感类别的索引
      >
      > 2-10表示构建情感类别索列表：C是自定义情感的类别数，论文中为3，首先遍历每个情感类别（`i` 从 1 到 `C`），并为每个类别构建一个索引列表 `˜Lt`；其次，对于每个文本样本 `j`，如果其情感类别为当前类别（`Lc[i][j] == 0`），则将该样本的索引 `j` 添加到 `˜Lt` 中，最后，将 `˜Lt` 添加到 `Lt` 中，`Lt` 最终包含每个情感类别的索引列表
      >
      > 11表示多模态的特征融合，对文本T和图像I进行特征融合，得到融合后的表示R
      >
      > 后续的步骤分别为：
      >
      > - `˜lpn` 是通过 `einsum` 操作计算的矩阵乘积，表示样本之间的相似度矩阵
      > - `lpn` 是对 `˜lpn` 进行 `LogSoftmax` 操作，并将其展平为一维向量
      > - `Lcl` 初始化为第一个样本的情感类别索引列表 `Lt[L[1]]`
      > - 然后，遍历剩余的样本（`q` 从 2 到 `S`（S表示L的长度）），将每个样本的情感类别索引列表 `Lt[L[q]]` 与当前索引列表 `Lcl` 进行拼接，并加上偏移量 `q × T`
      > - 最后，通过 `gather` 操作从 `lpn` 中提取对应索引的值，并计算平均值 `Llbcl`，作为最终的标签对比学习损失
    
    - 基于数据的对比学习，主要使用了数据增强
    
      考虑文字和图像的灵活表达，这可能会导致模型对数据的表面特征过于敏感，而不是专注于融合文本和图像中的不变特征，即有效特征。基于数据的对比学习可以强制模型学习数据中的有效特征，更有利于模型学习数据中与情感相关的特征，增强了模型对数据的鲁棒性，增强模型对数据中不变特征的学习能力
      
      对于文本的数据增强，就是通过将文本先翻译成其他语言，再将文本翻译回来，回译可以保留原始句子语义的同时生成多种释义，因此对比学习中采用回译法来构建文本的正例
      
      对于图片的数据增强，使用一种随机增强的方法不使用搜索，而是从相同的增强变换中集中统一采样
      
      用于数据对比学习（Data Contrastive Learning）的损失函数计算过程的伪代码：
      
      ![image-20241125134640827](..\images\image-20241125134640827.png)
      
      > 1表示使用多层融合模型（Multi-Layer Fusion Model）对原始文本 `T` 和图像 `I` 进行融合，得到融合后的表示 `R`
      >
      > 2表示数据增强之后的多模态融合模型：
      >
      > - `BT(T)` 表示对文本 `T` 进行回译（Back-Translation），这是一种常见的文本数据增强方法
      > - `RA(I)` 表示对图像 `I` 进行随机增强（RandAugment），这是一种常见的图像数据增强方法
      > - `MLF(BT(T), RA(I))` 表示使用多层融合模型对增强后的文本和图像进行融合，得到增强后的融合表示 `Rau`
      >
      > 3表示计算对比学习损失
      >
      > - `einsum(nc, ck− > nk, [R, RTau])` 表示使用爱因斯坦求和约定（Einstein Summation Convention）计算矩阵乘积，其中 `R` 和 `RTau` 是原始和增强后的融合表示
      > - `lpn` 是计算得到的相似度矩阵，表示原始样本和增强样本之间的相似度
      >
      > 4表示构建对比标签，`arange(S)` 生成一个从 0 到 `S-1` 的整数序列，表示每个样本的对比标签，这里的 `S` 是批量大小（batch size）
      >
      > 5表示计算交叉熵损失
      >
      > - `lpn/τ` 是对相似度矩阵 `lpn` 进行温度缩放（temperature scaling），其中 `τ` 是温度参数
      > - `Cross-Entropy(lpn/τ, cl_label)` 计算交叉熵损失，其中 `lpn/τ` 是预测的概率分布，`cl_label` 是真实的标签
      >
      > 6表示最后返回的数据对比学习损失
    
    基于标签的对比学习和基于数据的对比学习计算的损失可以简单地添加到总损失中作为正则化
    
    ![image-20241125140025561](..\images\image-20241125140025561.png)
    
    上述两种对比学习，主要的出发点都是使相同的情感数据相互靠近，使不同情感的数据相互远离

- 提出了两种基于标签和数据的对比学习任务，分别利用情感标签特征和数据扩充。这两个对比学习任务可以帮助模型学习多模态数据中与情感相关的共同特征，从而提高模型的性能（加入基于标签和数据的对比学习，可以进一步提高模型的性能，即对比学习可以使模型学习到关于情感的共同特征，并使不同情感数据相互远离）

***

文章亮点：文章在研究情感分析任务时，还设计了两个对比学习任务：基于标签的对比学习（`LBCL`）和基于数据的对比学习任务（`DBCL`），有助于模型学习共同的特征，大大提高模型的性能

开源的代码仓库：https://github.com/Link-Li/CLMLF

![image-20241126121244938](..\images\image-20241126121244938.png)

***

自我想法：结合基于标签的对比学习，可以把标签的思想使用到镁化炉中，镁化炉中有label数据，标记了正常和异常状态：正常状态（0）和异常状态（1）通过进行特征融合`MLF`多层融合模型，时序电流的特征为T，视频模态的特征为I，最后计算标记对比学习损失



## 相关知识

### `VIT`

![image-20241121151921932](..\images\image-20241121151921932.png)

`vit`全称为`Vision Transformer`，将图像视为一个序列化的输入，并利用自注意力来处理图像中的像素关系，将`Transformer`应用于图像图块（`patch`）序列上

`vit`可以分为两部分：1、特征提取；2、分类部分

结合研究的方向，这里主要介绍一下`vit`中的特征提取部分

特征提取：

1. 将输入的图片进行切割，进行分块处理，每一个小块就是对应一个`patch`，原文图中，将输入的图片切割为9个小块，就表示有9个`patch`，将切好的图片从左到右，从上到下进行依次排列，这样就将图像转换成一维的问题了（但是此时不是一个严格的一维）

   > `patch embedding`：如输入的图片大小为`224*224`，将图片分为固定大小的`patch`，`patch`的大小为`16*16`，那么每张图像会生成`(224*224)/(16*16)=196`个`patch`，即输入序列的长度为196，每个`patch`的维度为`16*16*3=768`，线性投射层的维度为`768*N(N=768)`，因此输入通过线性投射层后的维度依然为`196*768`，即一共有196个token，每个token的维度为768，这里还需要加上一个特殊字符串cls（要和其他图片进行一个区分），因此最终的维度是`197*768`，这样通过`patch embedding`将一个视觉问题转化为了一个`seq2seq`问题

2. `positional embedding`：`vit`同样需要加入位置编码，位置编码可以理解为一张表，表一共有N行，N的大小和输入序列长度相同，每一行代表一个向量，向量的维度和输入序列`embedding`的维度相同，位置编码的操作是sum，而不是concat，加入位置编码信息后，维度依然是`197*768`

3. 最后输入到`Transformer Encoder`中的内容必须是一维的向量

4. `LN/multi-head attention/LN`：`LN`的输出维度依然是`197*768`，多头自注意力时，先将输入映射到q、k、v，如果只有一个头，qkv的维度都是`197*768`，如果有12个头，（768/12=64），qkv的维度是`197*64`，一共有12组qkv，最后在将12组qkv的输出拼接起来，输出维度是`197*768`，然后再过一层`LN`，最后的维度依然是`197*768`

***

### 多模态大模型

#### `Flamingo`

要实现多模态对齐，需要进行预训练，预训练需要通过人工标注的标签进行训练

多模态大模型，处理各种模态我们需要首先考虑处理哪个模态为核心，一般是处理图像模态为核心的，其他特征，比如文本特征，一般是拉一个其他的大模型来即可，图像特征怎么映射到文本特征中去，是一个重要的考虑点，映射的越好，效果就越好

因此，视觉编码器非常重要，一般是使用`vit`相关的东西，如果是视频数据集，还需要考虑时间的维度

> 每个图像经过`vision encoder`会生成一个[S, d]的视觉特征，`T`个图像对应xf的维度为[T, S, d]
>
> 对于视频模态的数据，在vit中提取特征，需要加上一个time_embeddings，xf加上维度为[T, 1, d]的time_embeddings，之后再将时间和空间维度拉平xf->[T*S, d]
>
> 之后将xf作为transformer block的key 和 value输入
>
> 图像提供Q，文本提供K和V，这样的效果更好

#### `BLIP-2`

`BLIP-2`采用了`Q-Former`（`Query Transformer`）来弥补`image`模态和`text`模型的差距，实现特征对齐

`Q-Former`是一个双塔模型，两边的模态特征都可以兼顾到，`image`的部分提供Q，`text`的部分提供K和V

***

### 对比学习

对比学习是通过正负样本对互相对比来学习样本的特征

- 对比学习是一种无监督的学习，通过正负样本对，来学习样本的特征表示
- 在特征空间里正样本的特征尽可能的靠近，负样本的特征尽可能远离
- 对比学习是没有固定label的，随着模型训练，同一个样本生成的特征一直在变，监督学习都是有固定label的
- 对比学习要实现效果好，需要满足两个条件：1、负例要尽可能的多；2、负例要尽可能一致
- 常见的对比学习有`clip`、`moco`

### 监督学习





## 2024/11/26 多模态方向的讲座

特征融合方面：发展过程

1. 张量融合

   - 张量融合网络TFN（2017）：多种模态张量进行张量内积计算

     经典张量融合算法，但是内积运算计算量大

   - 低秩多模态融合网络LMF（2018）：低秩分解权重矩阵为模态特定低秩因子向量，并与不同模态张量进行线性变换后再进行多维度乘积

     有效的缓解了TFN计算量的问题，但是性能一般

   - 高阶多项式融合网络（2019）：多模态张量拼接后，通过LMF结构实现高阶张量内积运算

     TFN和LMF的综合，提取高阶多模态特征，但是内积运算在多模态交互关系提取方面有限，性能受限

   总之，张量融合方法在计算效率和性能之间难以取得良好的平衡，我们需要探索计算量低，效率高的融合机制

2. 多模态共享表征学习：训练过程通过特定的损失函数约束模型，来学习多模态共享特征

   - 多模态情感识别端对端网络（2019）：采用最大相关性损失和基于自编码器的重构损失来学习多模态共享特征和个性特征

     早期通过损失函数学习共享特征的模型结构简单，复现性强，但是隐式的模态个性特征提取方案，个性与共性特征难以分离，导致性能一般

   - 模态不变-模态个性表征学习网络MISA（2020）：分别通过中心矩差异损失和差异损失将多模态特征分解为模态不变特征和模态个性特征

     显示提取模态，有效的提取个性特征和共享特征，但是损失函数数量多，存在权重重新分配问题

   多模态共享表征学习方法在损失权重分配上缺乏灵活性，我们需要探索灵活的融合机制

3. 注意力融合：通过注意力机制调整模态特征的权重或整合所有模态特征，有效建模复杂的多模态关系，且识别与泛化性能强

   - 软注意力机制（2019-2022）通过软注意力来动态调整不同模态特征的权重

     结构简单，权重学习可跟随网络训练优化，但是融合方式不灵活，标量型权重容量是引入冗余特征或抑制重要特征，矩阵型权重则较为庞大且计算量大

   - 多头注意力（2020-2021）：计算模态之间特征相似性，并作为权重整合多模态特征

     细粒度信息提取，但是各模态特征需具备相同的维度尺寸，且相似性矩阵计算缺乏灵活性

   - Transformer（2021-2023）：通过堆叠多层多头注意力，实现不同模态特征的多次融合

     有效提取深层多模态信息，性能好，但是，参数量和计算量大，且相似性矩阵计算缺乏自适应性

   注意力融合方法的模态交互计算难以表征复杂多变的模态特征，我们需要探索自适应交互融合机制

关键问题：如何有效融合多模态异构结构

对于模态数为2的跨模态融合，提出了双模态自适应交互注意力网络`AIA-Net`（设计搞笑、自适应的差异多模态特征融合策略，减少来自低贡献模态的冗余特征；设计有效的多层多模态融合结构，实现情感表达的深度和交互演化）

面向跨模态图像生成的统一反馈学习`UniFL`



## `Achieving Cross Modal Generalization with Multimodal Unified Representation`

文章题目：Achieving Cross Modal Generalization with Multimodal Unified Representation（用多模态统一表示实现跨模态综合）

***

研究背景：不同模态的标签成本可能会有很大差异，导致会出现只有一部分模态被标记，而其他模态未标记

***

研究现状：

- 现有的多模态表示学习方法更多地关注于粗粒度对齐或依赖于来自不同模态的信息完全对齐的假设，这在现实场景中是不现实的

- 许多研究探索了将不同的多模态信息整合到统一的语义空间中，这可以分为两种类型：隐式表征和显式表征

  对于隐式表示，有几种方法利用模态不可知编码器来表示不同的模态，或者采用对比学习来使不同的模态在高维语义空间中更接近

  对于显式表示，旨在使用统一的码本或原型来表示不同的模态，作为桥梁以促进跨模态的鲁棒对齐，此外，离散空间的使用，允许在高维空间中聚集类似的输入特征，从而能够以减少的潜在代码的数量来实现复杂的特征表示

  然而，这些研究主要是在量化之前将模态内的顺序特征压缩到单个向量中，或者依赖于来自不同模态的信息完全对齐的假设。因此，这些方法通常限于简单的任务，如跨模态检索，并在更复杂的情况下表现出较差的性能

***

文章贡献：提出了一种新的跨模态综合方法，即跨模态综合（跨模态泛化）（CMG），它解决了在预训练过程中从成对的多模态数据中学习统一的离散表示的问题，然后在后续任务中，当只标注一个模态时，该模型在其他模态中也能达到零次泛化能力

文章中提出了单码，它包含两个关键贡献：双跨模态信息解缠（DCID）模块和多模态指数移动平均（MM-EMA），这些方法促进了模态之间的双向监控，并在共享的离散潜在空间中对齐语义等价信息，从而实现了多模态序列的细粒度统一表示

文章的目标是将从标记模态获得的知识转移到下游任务中的其他未知模态，使模型能够有效地泛化，促进基于已知模态的跨模态知识转移

论文主要集中在实现一个细粒度的，统一的表示多模态序列，这个问题一般涉及到两个方面：

1. 在不同模态中提取具有相同语义的信息，同时减轻模态特定细节的影响（是跨模态泛化的关键）

   提出了双跨模态信息分解（DCID）模块，其将对比对数比上限（CLUB）与本文提出的跨模态对比预测编码（Cross-CPC）相结合，具体而言

   - 使用CLUB优化互信息的上界，有效地区分模态无关的语义信息（传达主要内容或事件）和模态特定的信息（与语义无关的附加细节，如视觉中的光线和透视，或音频中的音色和音高等）

   - 然而，如果没有有效的指导，模型很难识别语义信息的哪些方面是有用的。文章提出了一种新的Cross-CPC方法，根据当前模态的已知序列信息预测其他模态中的未来信息，可以有效地最大化不同模态之间的细粒度互信息

2. 使用统一的码本表示具有共享语义的这些不同模态（之前大多数工作的重点）

   提出了多模态指数移动平均（MM-EMA），它采用了教师--学生机制，以促进预训练前的跨模态指导，该机制鼓励将从具有共享语义的不同模态编码的量化向量聚合到相同的潜在代码空间中。否则，不同的模态将聚集在离散空间的不同区域中，而不是映射在一起（主要是针对预训练前数据对没有对齐的情况，使用提出的MM-EMA将数据先进行对齐后，再进行后续的预训练）

文章的贡献主要涉及到：

1. 引入了CMG任务，它增强了现有模型在多模态学习领域的适用性，解决了模态稀缺和标注某些模态的巨大成本所带来的挑战
2. 提出了一种新的语义映射框架Uni-Code，该框架能够有效地从成对的多模态数据中提取出共享的语义信息，并将其细粒度地投影到一个公共的量化潜在空间中

***

背景知识综述：

- 隐式多模态统一表示，目的是在共享的潜在空间内对齐不同的模态或尝试学习模态不可知的编码器，以提取各种模态的信息，这些方法研究了各种模态组合的统一表示，其方法包括：跨模态知识蒸馏以跨模态转移知识；基于CLIP的方法使用对比损失从大型配对数据集中学习图像-文本一致性等等
- 显式多模态统一表示，通过利用通用的codebook或原型来显式表达多模态内容来实现多模态显式统一表示，在本文中，主要研究如何映射一个成对的多模态序列到一个共同的离散语义空间，在多模态序列中的信息是不受约束的，可能不是完美的对齐
- 互信息MI估计，旨在度量两个随机变量之间的相关性，为了最大化序列中的互信息，对比预测编码（CPC）采用自回归模型和对比估计来捕获长期关系，同时保持序列内的局部特征，互信息最小化则试图减少两个随机变量之间的依赖性，同时保留相关信息，已成功应用于解纠缠表示学习，对比对数比上限（CLUB），将互信息估计与对比学习相结合，以近似互信息上限，工作更适合互信息的最小化

模型结构：

![image-20241130134048602](..\images\image-20241130134048602.png)

上图的模型是文章提出的统一代码框架概述，使用视听作为示例：（a）表示模型的主要管道，（b）表示多模态在矢量量化过程中，包含了多模态EMA（MM-EMA）和一个新的承诺损失，注意到大量的离散码将被重置，（c）是文章中提出的Cross-CPC架构

- 跨模态泛化任务：架构中给定的是一组成对的多模态数据，其中A、B、C等表示不同的模态数据，跨模态泛化（CMG）任务旨在在预训练阶段期间将这些各种模态映射到统一的离散空间中，使得离散潜在代码能够在具有相同语义的不同模态之间共享。随后，在下游任务中，当仅一种模态（例如，模式A）具有注释（label）的信息，则该模型可以将从A模式学到的知识转移到其他模态（例如，模式B和C），以实现零触发泛化能力
- 统一表示学习：统一表示的成功在于提取模态不可知的语义特征，而不是简单地从成对模态中提取信息，然后直接映射，本文从两方面实现这一点：
  1. 引入了一个DCID模块，设计用于提取细粒度的语义信息，并将其与每个模态中对应的模态特定信息分离
  2. 通过VQ-VAE将提取的语义特征压缩为离散变量，保证压缩后的离散变量经过重构损失后仍能包含原始语义信息

对于两个成对的模态：

1. 使用两个语义编码器提取模态无关的特征，并使用两个模态特定的编码器分别从模态A和B中提取剩余的特征![image-20241130141735342](D:\Myproject\develop-study-notes\images\image-20241130141735342.png)前面两对表示模态的无关特征，后面两对表示模态的剩余特征（相关特征)

2. 使用向量量化运算将语义特征映射到细粒度级别的离散潜码，在模态A和B之间共享潜在码本，最后组合回一起重建原始特征

3. 在这项工作中，使用指数移动平均线（EMA）来代替VQ损失，因为EMA更强大，在理想条件下，从具有相同语义的不同模态编码的zai和zbi应该被映射到相同的离散潜在代码

4. 同时需要引入一些模块来促使上述工作的实现

   1. 双跨模态信息解缠

      在预训练的过程中，对逼近网络和主网络交替进行优化

   2. 多模态指数移动平均线

   3. 预训练和下游任务

***

### 补充知识

#### Prototypical Network



#### `VQ-VAE`

`AE`（自编码器）模型：能够将图像压缩成一个向量的神经网络模型，包括一个编码器和解码器，在训练的时候将输入的图像`X`，训练成一个较短的向量`Z`，然后再解码成另一个长得差不多的图像，使与输入的图像尽可能的相似，解码器的作用是可以将一个向量解码成一张图片（换言之，解码器就是一个图像生成模型：可以根据一个向量来生成一个图像），但是AE模型是不能做图像生成的，因为`AE`的解码器只认识从`AE`编码器出来的向量，如果我们输入一个随机的向量给解码器，解码器是输出不了图像的，总之，`AE`是一类能够把图片压缩成较短的向量的神经网络模型

`VAE`模型是一个生成式的模型，解除了`AE`模型的一个限制，在`AE`的基础上将编码器的编码空间限制的比较工整，符合某个数学的分布（如高斯分布），后续我们可以从高斯分布中随机采样一组向量，输入到解码器中，就能生成一个图像，在训练完后，我们可以将我们的编码器去掉，使用高斯分布的随机向量输入解码器进行图像的生成，但是，通过`VAE`生成的图像效果并不是很好，图像质量并不高，因为图像经过编码，输送到连续的向量，而将图像编码成离散的向量（变量值可以按一定顺序一一列举，通常以整数位取值（0、1、2...）的变量，这种变量的取值是有限个值，且其取值都是以整数位断开的，具有最小计量单位）往往会更加的自然，离散的变量更符合我们的一个生成的规律

VQ-VAE模型首个提出 codebook 机制的生成模型，利用codebook机制把图像编码成离散向量，为图像生成类任务提供了一种新的思路，使用离散的变量代表图像中的一个特征，VQ-VAE模型在VAE的基础上将编码器输出的向量进行一个离散化，得到一个离散化的向量，在通过解码器去获得一个随机的图像

![image-20241130154820029](..\images\image-20241130154820029.png)

但是将图像编码成离散向量后会带来两个问题：

1. 神经网络会默认输入一个连续的分布，并不善于处理离散的输入，对于输入0，1，2三个离散的输入，神经网络会默认输入的1是0和2之间的一种中间状态

   为了解决这个情况，作者使用了NLP模型中对离散单词的处理方式，NLP模型的第一层一般都是词嵌入层，它可以把每个输入单词都映射到一个独一无二的连续向量。这样，每个离散的数字都变成了一个特别的连续向量了，如man就可以被映射成一个连续的向量，将类似的映射（嵌入层）添加到VQ-VAE的解码器之前，这个嵌入层在VQ-VAE里叫做"embedding space（嵌入空间）"，在后续文章中则被称作"codebook"

   ![image-20241130155329354](..\images\image-20241130155329354.png)

   VQ-VAE编码器输出一个离散的情况，再通过嵌入空间codebook，使其变成连续的值，再通过解码器去解码出图像

2. 离散变量不好采样

   VQ-VAE的最终目的是抛弃掉编码器，通过随机的生成/编码出一组离散的向量，然后通过嵌入空间输出连续的向量，最后去解码出一张图片，但是这种离散变量是不好采样的

   ![image-20241215151910916](..\images\image-20241215151910916.png)

   作者使用了PixelCNN对VQ-VAE的离散空间进行采样，PixelCNN能够拟合出一种离散的分布，对于一个图像，PixelCNN能够输出某个像素值，某个像素颜色通道取0-255中某个值的概率分布

VQ-VAE和AE的唯一区别，就是VQ-VAE会编码出离散向量，而AE会编码出连续向量

VQ-VAE的工作过程：

1. 训练VQ-VAE的编码器和解码器，使得VQ-VAE能把图像变成「小图像」，也能把「小图像」变回图像
2. 训练PixelCNN，让它学习怎么生成「小图像」（输出一个离散的颜色值，学习怎么生成离散的编码空间）
3. 随机采样时，先用PixelCNN采样出「小图像」（离散编码空间），再用VQ-VAE把「小图像」翻译成最终的生成图像

VQ-VAE的编码器怎么输出离散向量：

![image-20241215152507195](..\images\image-20241215152507195.png)

最简单的方式是和多分类模型一样，输出一个softmax的概率分布，之后从概率分布中随机采样一个类别，这个类别的序号就是我们想要的一个整数，但是这么做不是一个高效的方式，可以直接将输出编码器的张量logit和解码器的输入张量embedding和嵌入空间codebook做一个关联，使用logit和embedding去组合我们的嵌入空间，如何在嵌入空间中进行一个采样？用到的方式是求最近邻

![image-20241215153512814](..\images\image-20241215153512814.png)

首先通过编码器，得到其输出ze(x)，在通过嵌入空间还原成为一个连续的向量zq(x)，最后在去解码，这一个过程涉及到求最近邻的方式

![image-20241215154001494](..\images\image-20241215154001494.png)

停止梯度的运算：前向传播和反向传播的计算可以不对应，可以为每个运算设计求梯度的方式

![image-20241215190055307](..\images\image-20241215190055307.png)

VQ-VAE有一个编码器和解码器，编码器会将图片变成`Ze(x)`，是一个`D`维的，同时有一个K*D维的字典，D是每一个元素的维度，一般设为512，K是字典中元素的个数（`codebook`的长度，可以简单的理解为`codebook`对于`K`个聚类中心），编码器的输出内容是`h*w*D`的特征图，有`h*w`个元素，将特征图的向量去与`codebook`中的`K`个向量进行比较（分别计算相似度），将最相似的`ei`的`index`存入特征矩阵`Z`中（对于每一个向量元素，我们可以在字典中找到离这个元素最近的元素，之后将这个指标进行记录下来，得到了一个`Z`），`Z`是一个`h*w`的矩阵，`Z`中的每一个元素，比如说第一个元素，就是编码器的第一个元素对应字典中离他最近的元素的位置，1表示字典中第一个位置离这个编码器元素的位置是最近的；我们可以使用`Z`重建出`Zq(x)`，重建的过程就是一个查字典的过程，按照字典的位置将向量放进去，放到对应的位置，重建的`Zq(x)`也是`h*w*D`，再经过一个解码器就能得到恢复的图像，该过程对应的公式为：

![image-20241215191511115](..\images\image-20241215191511115.png)

> 得到的`z`是以我们字典中找到一个离我们的`ze`最近的那个元素，论文中写成的是一个`one-hot`的表达形式（理解成写对应的元素）

在做训练的时候是以下面的损失来进行计算的：

![image-20241215191742212](..\images\image-20241215191742212.png)

> - 第一个损失表示一个重建损失，是一个经典的损失，只训练`encoder`和`decoder`的，是不训练字典的，通过最小化重构模型输入`X`以及`Decoder`部分
>
>   > `Encoder`输出的每一个词向量需要从`codebook`中找到最相似的词向量来代替他送入`Decoder`完成计算。这个“选择”的过程自然是不可导的，也就是说，目前的计算方式会导致`Encoder`无法计算自己的梯度，自然无法训练
>   >
>   > 将原本能够正常计算出来的`codebook`身上的梯度`dL/dZq`直接作为`Encoder`的梯度，即`dL/dZe`
>   >
>   > 重建损失衡量的是输入数据与解码器输出之间的差异，具体来说，它表示输入数据经过编码器、量化和解码器后，重建的特征与原始输入之间的差异
>
> - 第二个`VQ`损失是用来训练字典的`codebook loss`，衡量编码器的输出与码本中码字`e`之间的差异（编码器的输出经过量化后，与码本中的码字之间的差异），把`Ze(x)`进行冻结，表示梯度不传播（即在前向计算的时候保持相应的量不变，但在后向计算的时候使得梯度为0（也即`pytorch`中的`detach`方法）），对于这个损失来说，`Ze(x)`相当于一个常数，而`e`是一个字典
>
> - 如果只加第二个损失，字典中的K是没有进行限制的，`K`可以非常的长，所有就有了第三个损失，冻结字典`e`，将其看成一个常数，在更新`Ze(x)`，希望对于`encoder`中的这个元素尽可能的忠实于字典中的某个元素，不要进行反复横跳（让`Encoder`的输出稳定在一个`codebook`聚类，不在`codebook`里面乱跳，作者通过实验发现最后的结果对于`β`并不敏感，故设置了一个0.25的值)，该损失主要衡量编码器输出与量化后的码字之间的差异，但码字部分不参与梯度计算

上面两个公式对应VQ-VAE的训练过程，对于重构的过程，对于得到的`Z`，通过查字典的方式得到一个`Zq(x)`，最后通过一个`decoder`就能得到一张图片



#### `CPC`

对比预测编码（`CPC, Contrastive Predictive Coding`）是一种自监督的方法，可以从高维数据中提取有用的表示信息` representation`，这种表示信息 `representation `学习到了对预测未来最有用的信息，`CPC` 的核心思想是通过对比学习（`Contrastive Learning`）来预测未来的数据点，从而捕捉数据中的时间依赖性和语义信息，`CPC`是对比学习方法的一种（让像的样本差异尽可能的小，让不像的样本差异尽可能的大）

算法的核心是通过强大的自回归模型来学习未来（预测的）隐变量表示；使用了对比损失概率来引入最大化预测样本的信息和隐变量；对负样本进行了采样使模型更易于处理

互信息`MI`表示两个变量之间的相关性，`I(x;c)`表示`x`与`c`的关系，互信息越大，就表示`x`和`c`是越接近的，两者的相关性越大
