# 交流记录

## 2024/10/31  第一次交流

吴老师，我最近看了一些关于CLIP的内容，结合着想了一下镁化炉数据多模态对齐：

我觉得对齐多模态数据主要考虑的是，为了可以精确的得到这个电流数据的预测提前区

我认为对齐需要拿出一个异常周期的视频数据和时序电流数据（电流的数据可以往前拿多一点，因为电流相较于视频会提前产生异常）（也不能拿多个异常周期，因为在后续对齐时，可能前一个电流的异常数据和后一个视频的异常数据计算的相似度也会比较高，这个情况是需要避免的）

难点：像传统的CLIP，使用的是图片模态和文本模态，对应的文本模态间内容的差异往往是比较大的（比如输入的文本内容是：这是汽车、这是飞机），这样与图片模态特征数据进行的相似度计算的结果往往可以较好的体现差距（正样本和负样本是比较好区分的），但是如果使用镁化炉的数据，这个的序列数据的特征差异往往是比较小的（异常程度当作具体的差异？异常、未异常、小异常、大异常？），我认为计算相似度差异的区分度可能不会有这么明显的效果（可能最后得到的矩阵各个位置的相似度值差距不大）

问题：

1. 电流数据的异常特征是如何体现的？（没有看过具体的数据，不是很清楚）
2. 你给我的思路图中的`ev`、`ec`中的每个单元是不是表示一个时刻的视频数据模态特征和时序电流数据模态特征？

***

交流反馈：

- 电流的异常会比较反常，波动很大的时候也很可能是正常的，这个很难去通过经验来判断，所以还是需要通过视觉

- 确实像你说的，需要把电流提前取比较多的值，同时也避免取过多。比如一般的异常是电流3-5分钟后视频会产生异常，那么我们取的电流信息就需要覆盖5分钟之前即可；

- 异常不分大小，只是binary的分类（二分类）问题；

- 图中的`ev` `ec`指的是特征，输入包含了时序信息



## 2024/11/09  第二次交流

吴老师，我这段时间主要了解了一下你发我的数据（拿这些数据尝试去进行简单的特征提取，试了一些transformer相关的特征提取代码）和一些关于对比学习相关的内容

- 对于对比学习，训练一个像CLIP这样的大模型是不太可能的（该模型训练所需的数据量是很大），但是我觉得CLIP这个模型的思路是可以借鉴的，通过余弦相似度来计算两个模态特征的相关性

- 模型初步设计想法：在数据集中找出一段比较有代表性的连续数据，考虑50个时间步长的视频模态数据，100个时间步长的时序电流模态数据（后50个时间步长的数据与视频模态的数据保持时间上的对齐），时序电流模态上做一个大小为50的滑动窗口，视频模态上做一个大小为50的固定窗口，分别提取窗口中两个模态的特征，计算模态间的余弦相似度，随着滑动窗口的不断滑动，找到两个模态相似度最高的位置（相似度矩阵的对角线上都是正样本即可，因为是二分类的问题，除了对角线的其他位置还是会有正样本的），从而得到时序电流的提前时间。

  但是上面这个方法细想有点问题：在提取特征的时候，需不需要将异常/正常标签label的数据信息放到各个模态中一起进行特征的提取，但是这样时序电流数据模态是没有准确的异常/正常标签的（感觉数据集里面的label只是标注了视频模态数据的异常/正常信息，对于时序电流数据没有额外的标注说明，那么后面如果使用对比学习进行训练的时候，是不是可以理解为当三相电流在短时间内有较大的上升时，就认为发生了异常），如果需要融入label到各个模态中，是要通过怎么个方式进行融入呢？还是说不需要label的信息加入到特征中，只要通过视频模态的特征来对齐时序电流的模态即可？（是不是可以认为电流忽然较大幅度的上升和视频中`rgb`颜色变成红色/黄色/白色，这两个特征认为是相似度比较高）

  还有一个就是数据中的一个时间步长（两次间隔的采样）具体是多少时间？

个人思路：

![image-20241122144203595](..\images\image-20241122144203595.png)

***

交流反馈：

个人提出的思路是可以的，但是要重点关注怎么从特征层面上进行多模态的对齐，多去了解一下`cross-modal alignment`或者`multi-modal alignment`的文章



## 2024/11/22 第三次交流

吴老师，我这段时间主要看了一些跨模态对齐相关的文章，梳理了一些有代表性的对比模型和问题，主要涉及到模态间的对齐方法、特征间的模态融合和对不同数据模态的特征提取

可借鉴的对比模型：

- **模态间的对齐**，在`BLIP-2`大模型中，使用了`Q-Former`（`Query Transformer`）来弥补模态间的差距，实现了特征的对齐，具体而言就是一个模态提供Q，另一个模态提供K和V，对齐实现方式是通过一个模态的K和V去查找另一个模态的Q（这种对齐方式是一个全局上的模态对齐，但是忽略了模态间的时间特性）

- `AlignVSR`模型用来对齐视频模态的数据和音频模态的数据，采用了全局的对齐和局部的对齐方式：

  ![image-20241121110434214](..\images\image-20241121110434214.png)

  - 全局对齐（类似于`Q-Former`的方式）：对准的实现从每个视频帧到音频单元组的交叉注意力过程，即视频特征是查询（Q），而音频单元作用键（K）和值（V），这种对齐是全局的对齐，视频可以与任何音频单元进行对齐（为了更好地保留视频的时间信息，在视频特征中必须涉及位置编码）
  - 局部对齐：全局对齐不利用音频和视频模态之间的时间对齐，因此设计了局部对齐机制来解决这个问题，图中u表示音频单元经量化的数目，v表示视频特征序列，a表示音频特征序列（由于采样频率不同，一个视频帧对应三个音频帧），T表示是视频帧的总数

- 对于**模态间特征的融合**，在一篇情感分析的文章里面，提出了一种基于`Transformer-Encoder`的多层融合模型，利用多方向自注意的思想实现了文本和图像的标记级特征的对齐和融合，并利用`MLF`进行特征融合，从而提高了模型的抽象能力

  ![image-20241119160843178](..\images\image-20241119160843178-1732265824593-1.png)

  `MLF`多层融合模块，使用文本（`BERT`）-图像（`ResNet`）编码器来获得文本和图像的隐藏表示（提取特征），之后需要将图像特征的维度转化为与文本特征相同的维度，将图像特征输入到基于多层`Transformer-Encoder`的图像`Transformer`层，得到图像序列特征的最终编码

  对于特征的融合，将文本的特征与图像序列特征连接起来，再使用一个多层变换器编码器作为文本图像融合层，对齐和融合多模态特征，得到文本和图像的融合序列特征

  ![image-20241120100248113](..\images\image-20241120100248113.png)

  在获得了文本和图像融合的序列特征后，序列特征不能用于分类任务，因此，我们使用一个简单的注意力层来获得多模态表示

  这个模型中还提出了基于标签的对比学习（`LBCL`）和基于数据的对比学习任务（`DBCL`），其中基于标签的对比学习我认为是可以进行借鉴的：

  文章中的基于标签的对比学习，是根据情感标签将每个批次中的数据分为正面和负面示例，对于多模态数据的负标签，批次中具有相同负标签的数据作为正例（粉红色的正方形），而没有负标签的数据作为负例（灰色的正方形），该算法主要包括两个步骤：第一步是根据批量中的数据标签生成去掩码标签；第二步是计算损失矩阵，利用去掩码标签和损失矩阵得到最终的损失

  **结合基于标签的对比学习，我觉得可以把标签的思想使用到镁化炉中，镁化炉中有label数据，标记了正常和异常状态：正常状态（0）和异常状态（1）通过进行特征融合`MLF`多层融合模型，时序电流的特征为T，视频模态的特征为I，最后计算标记对比学习损失**

- **对于不同模态数据的特征提取**，`OneLLM`大模型是一种使用统一框架将八种模态与语言相结合的`MLLM`，通过一个统一的多模式编码器和一个渐进的多模式对齐管道来实现这一点

  ![image-20241122181748879](..\images\image-20241122181748879.png)

  上图为`OneLLM`架构，该架构由模态标记化器、通用编码器、通用投影模块（`UPM`）和`LLM`组成，其中：

  - 模态标记器是`2D/1D`卷积层，用于将输入信号变换成标记序列

  - **通用编码器`Universal Encoder`**是用于提取高维特征的冻结视觉语言模型

    使用`CLIPViT`作为通用计算引擎，对于视频信号，将所有视频帧并行送入编码器，并在帧之间执行令牌式平均以加快训练，对于标记拼接，可以进一步增强模型的视频理解能力

  - `UPM`由几个投影专家和模态路由器组成，以将输入信号与语言对齐。对于对齐阶段，训练模态标记器和`UPM`，并保持`LLM`冻结

问题：

- 对于多模态对齐方面的文章，感觉很多文章中都是基于大模型来实现的（基于这些大模型做下游任务），这些预训练好的大模型涉及到的数据集都是非常庞大的，而且算力的要求也是非常大的

  我觉得基于镁化炉数据集做出来的对齐模型是不是通用性没有这么好？（做出来的对比模型可能只是适用这个特定的数据集，感觉如果使用镁化炉的数据来进行对齐好像有点过于单薄，但是如果加一点公开的数据进行对比实验，类似的视频-时序电流模态的公开数据集不太好找，如果使用图片-文本的数据集来进行后续设计的模型的对比实验，感觉有点不搭，思路有点卡壳，希望老师可以指点一下）

- 对比学习要实现效果好，需要满足两个条件：1、负例要尽可能的多；2、负例要尽可能一致，我们的数据集应该如何有效的进行构建负例？什么样的模态特征可以算作一个正例？（视频区域的`rgb`颜色变黄/白和三相电流忽然大幅度升高/三相电流的三相数据差异较大）

***

交流反馈：

- 继续去了解情感分析那篇文章中的研究方法

- 不用考虑通用性，基于镁化炉的数据集做这个工业背景下的专项研究

- 采用特征拉远拉近的方式进行对比学习
- 数据集中的电流异常表现机理是：电流数据变化较小的说明发生了异常，电流数据变化较大的表示镁化炉正在工作（正常工作的情况下，电流变化是比较大的）这一点与之前的理解是有偏差的



## 2024/11/30 第四次交流

吴老师，我这个星期主要去复现和看了一下之前那篇情感分析论文中的代码，有一些体会和问题：

1. 对于那篇情感分析论文中的代码深入的了解了一下，我发现，它用到的基于标签的对比学习是一个自监督的对比学习，也就是说对于数据，是没有提前人工打标签标注的，是在训练的过程中以自监督的形式生成的情感标签
2. 从代码和思路图中看出，作者的重心主要是在跨模态融合和基于数据增强的对比学习方面，对于基于标签的对比学习基本是简单带过，我感觉基于标签的对比学习更像是一个模型最后的优化实验（加入了这个模块，稍微提升了一点性能指标）
3. 感觉基于自监督的对比学习不太适合镁化炉这个工业场景，而且原论文中的实际模态（图片和文本模态）和我们的数据模态差别挺大的，于是我沿着基于监督的对比学习和基于时序数据的对比学习去找相关论文

我找到了这么一篇文章：

`Achieving Cross Modal Generalization with Multimodal Unified Representation`

这篇文章提出了一种新的跨模态综合方法，即跨模态综合（跨模态泛化）（`CMG`），它解决了在预训练过程中从成对的多模态数据中学习统一的离散表示的问题，然后在下游任务中，**当只标注一个模态时，该模型在其他模态中也能达到`zero-shot`泛化能力**

文章的重点在于研究如何实现细粒度层面上的多模态序列统一表达，提出了多模态 EMA，利用 teacher-student 机制，让不同模态相互指导，在离散空间中互相靠近，并最终将具有相同语义的不同模态变量收敛到一起

![image-20241130134048602](..\images\image-20241130134048602.png)

通过预训练获得了一个多模态通用的 codebook，以及能将这些模态映射到该离散空间的对应模态的编码器，在下游任务中，利用预训练得到的编码器和 codebook，只对一种模态进行训练，然后让将训练得到的模型零样本的迁移到其他未知模态中进行测试，测试的任务包括跨模态事件分类，跨模态事件定位，跨模态视频分类和跨模态检索

模态的具体方法：

1. 给定两个成对的模态数据![image-20241130182122898](..\images\image-20241130182122898.png)，使用两个语义编码器去提取模态的无关特征，使用两个特定模态数据提取编码器去提取模态的剩余特征：

   ![image-20241130182401296](..\images\image-20241130182401296.png)

2. 对语义特征使用 `VQ-VAE `在细粒度层面上对模态信息进行离散化，使用了同一个 `codebook`，然后将离散化后的向量与模态无关向量合并后，重构回原来的特征，在本文中，使用指数平均移动（`EMA`）来代替 `VQ loss`，重构` loss` 则保证了压缩后的离散变量仍旧具有不同模态的语义信息，但是如果在没有监督的情况下，会出现不同模态的语义特征会收敛到 `codebook` 中的不同区域，难以实现下游的各种跨模态泛化任务，于是作者又提出了以下的模块来缓解这个问题：

   1. 对偶跨模态信息解耦（`DCID`）

      首先是在每个模态内部将` modal-agnostic semantic features` 与 `modal-specific features` 的互信息最小化（`CLUB`）（可以减少每种模态中语义信息和模态特定信息之间的相关性），其次是对不同模态中 `modal-agnostic semantic features`的互信息最大化（`Cross-CPC`）（`CPC`表示对比学习预测，被广泛的用于序列特征的自监督训练中，对于人类来说，我们可以根据当前模态中的已知序列去预测另一个模态中可能发生的情况，因此，本文提出了`Cross-CPC`，可以通过这种细粒度的跨模态对比预测将不同模态中相关的语义信息互相提取出来，有效地最大化不同模态之间的细粒度互信息）

   2. `Multi-modal Exponential Moving Average`

      提出了一个多模态 EMA 方法，能够让不同模态在离散化过程中，互相作为 `teacher-student`迭代更新，最终收敛到一起，促进模态间的对齐

我觉得这篇文章的立足点是比较符合我们的需求，**对于多模态的数据，从标记模态获得的知识转移到下游任务中的其他未知模态，使模型能够有效地泛化，促进基于已知模态的跨模态知识转移**

***

交流反馈：

- 这篇文章的思路是可以的，我们主要学习的是一个模态对齐的表征
- 这篇文章的写作思路与我们的实际问题是比较符合的，很多内容可以照着我们电熔镁炉的背景修改一下即可
- 给了电熔镁炉的背景知识，用于开题报告
- 给了多模态电熔镁炉异常检测的代码，用于参考



## 2024/12/11 第五次交流

我感觉这个模型的有些模块是值得借鉴的，特别是语义特征编码器提取模态的语义特征，将高层次的语义和上下文关系进行提取（因为我感觉对于我们的数据集如果使用专用的编码器去进行特征的提取，可能提取不了非常有用的特征，这些特征对后续的对齐可能没有很大的影响），还有`Cross-CPC`模块，对于没有完美匹配对齐的模态，使用了自回归模型总结了历史信息，模型可以通过对比学习在细粒度级别上预测其他模态下一步可能的信息

`CMG`模型总览：

![image-20241130134048602](..\images\image-20241130134048602.png)

> 在预训练阶段，通过跨模态泛化`CMG`，将不同模态映射到一个统一的离散空间中，使离散的潜在代码具有相同语义的不同模态之间共享
>
> 模型将多模态序列映射到一个共同的离散语义空间中，其中的多模态序列信息可能是不受约束的（也就是说模态间不是完美对齐的），文中使用`VGGSound24k`数据集训练视觉-听觉-文本的统一表示学习，经过训练，得到一个统一离散的潜在空间
>
> - 使用语义编码器提取模态的语义特征（语义信息是指数据中包含的高层抽象的概念或语义含义（包含了高层次语义和上下文关系等等），而不是低层次的视觉特征（如边缘、纹理和颜色等等）），再使用特定编码器提取模态的剩余特征
>
>   使用`DCID`模块提取细粒度的语义信息，并且将其与每个模态中相应的模态特定信息分开，双跨模式信息解缠
>
>   语义编码器提取模态的语义特征：结合了通道注意力机制（通过全局平均池化和线性变换生成通道注意力权重）和自注意力机制（通过全局平均特征和局部特征的交互，生成空间注意力权重）
>
> - 使用向量运算将语义特征映射到细粒度级别的离散表示
>
>   使用`VQ-VAE`模块将语义特征压缩为离散变量，确保压缩后的离散变量通过重构损失仍保持原始的语义信息
>
>   代码中定义了一个名为 `Cross_VQEmbeddingEMA_AVT` 的函数，用于对音频、视频和文本的语义特征进行向量量化`（Vector Quantization, VQ）`，并计算交叉模态的损失。该模块结合了指数移动平均`（Exponential Moving Average, EMA）`（使用指数移动平均`EMA`来代替`VQ`损失，因为`EMA`更加稳健，重建损失可以保证压缩后的潜在代码仍然保持不同模态的语义信息）和交叉模态一致性损失，以实现跨模态的特征对齐和量化
>
> - 在codebook中共享多个模态的信息
>
>   在理想情况下，对于相同语义的不同模态语义特征编码应该映射到相同的离散潜在代码中，但是如果没有有效的监督，模态差异将导致不同模态的语义特征会聚到codebook中的不同区域，因此，需要使用一定方式进行缓解：
>
>   采用互信息MI估计，衡量两个随机变量之间的依赖关系
>
>   - 互信息MI最大化：学习捕捉输入数据中有意义的和有用的数据表示，从而提高各种下游任务性能，不同模态中模态不可知语义特征之间的MI最大化
>
>     对比预测编码`CPC`采用回归模型和对比估计来捕获长期关系，同时保持序列中的局部特性，从而提高序列中的互信息最大化，`Cross-CPC`将`CPC`拓展到跨模态收缩预测，对于没有完美匹配对齐的模态，使用了自回归模型总结了历史信息，模型可以通过对比学习在细粒度级别上预测其他模态下一步可能的信息，文章中使用两个单层的`LSTM`来总结两个不同模态当前时刻之前的语义特征，得到两个模态的上下文表示，，然后使用一个模态的上下文表示预测另一个模态k个未来时间步
>
>   - 互信息MI最小化：减少两个随机变量之间的依赖关系，同时保留相关信息，多用于解缠表示学习，每个模态中不可知语义特征和模态特定特征的MI最小化（减少每种模态中语义信息和模态特定信息之间的相关性）
>
>     对比对数比上限`CLUB`将MI估计于对比学习相结合，以近似MI上限，`CLUB`可以有效的优化MI上界，在信息解缠方面表现优势，使用`CLUB`优化语义特征和模态特定特征
>
> - 最后将细粒度级别的模态信息与剩余特征组合回一起，重建原始特征
>
> 将预训练好的多模态编码器应用于下游的任务，包括：跨模态事件分类（音频和视频中都有一个主要事件，使用单一的模态训练事件分类器，并直接评估另一个模态在当前分类器上的性能）、跨模态事件定位（部分数据和音频有精细的事件标签进行注释，在一种模态上进行事件定位，然后直接将模态转移到另一个模态进行测试，使用准确率评估性能）、基于查询的视频分割和跨域和模态事件定位，同时在执行下游任务期间，预训练编码器的参数是被冻结的
>
> 当只有一个模态A有注释信息时，模型可以根据预训练时获得的共享离散空间，将A模态中学到的知识转移到其他模态，实现零镜头泛化能力
>
> `CMG`模型方法的核心是区分和细化跨模态数据中的共享语义内容，同时消除特定于每种模态的冗余信息。这种设计使模型能够学习更精细和通用的跨模态表示，即使在模态之间没有完美对齐的情况下，也能够提取公共语义。因此，这种方法增强了模型在更复杂任务中的性能。

`vggsound-avel40k.csv`文件的内容：根据`split`字段将数据划分为训练集、测试集和验证集

![image-20241211204349337](..\images\image-20241211204349337.png)

具体使用的video和audio数据保存形式使用的是`.pkl`文件进行序列化保存的

`vggsoundCategories2Prompts.csv`：该文件包括了类别标签到文本提示的映射（对于类别事件给出了一段稍微具体的文本描述）

![image-20241211211750304](..\images\image-20241211211750304.png)

**我觉得`CMG`模型中是有许多知识值得去学习和研究的，如：使用语义编码器去提取语义特征（语义特征有着高层抽象的概念或语义含义），将语义特征进行离散化处理，通过codebook将不同模态的相同语义特征映射到同一个区域，对于没有完全对齐的模态，使用`Cross-CPC`跨模态收缩预测等等**

今年该论文的作者又在这篇论文的基础上提出了一篇新的论文：`Unlocking the Potential of Multimodal Unified Discrete Representation through Training-Free Codebook Optimization and Hierarchical Alignment`

`CMG`模型使用了双跨模态信息去纠缠（`DCID`）模型利用统一码本的方式，实现细粒度表示和跨模态泛化方面表现出了良好的效果，然而，该算法仍然受到所有通道的同等对待和对小事件信息的忽视的阻碍，导致不相关通道的干扰和在细粒度任务中的性能受限（限制1：在嵌入空间利用效率和对齐粒度方面存在局限性；限制2：可能会忽略特定模态特有的关键事件信息，从而限制模型在细粒度任务中的性能），于是作者又提出了一种不需训练的码本最佳化（`TOC`）方法，借由在统一空间中选择重要通道而不需重新训练，以提升模型效能。此外，还引入了分层双跨模态信息去纠缠（`H-DCID`）方法，将信息分离和对齐扩展到两个层次，捕获更多的跨模态细节

对于`CMG`模型在在嵌入空间利用效率和对齐粒度方面存在局限性，作者引入了码本（`TOC`）的免训练优化机制来缓解这个问题，`TOC`通过计算优化码本，不需要额外的训练参数，从而增强在下游任务中计算和选择预训练模型的码本内的特征的能力。这种方法实现了无训练效果的改进，减少了额外的训练成本，并降低了下游任务的训练参数要求。

对于可能会忽略特定模态特有的关键事件信息，从而限制模型在细粒度任务中的性能这一个限制，作者提出了`H-DCID`模型（分层双交叉模态信息解纠缠），保留了`DCID`在实现统一多模态表达方面的优势，同时通过分层对齐机制显着增强了模型在处理细粒度信息方面的灵活性和精度，引入了二次对齐过程来有效地提取和对齐这些信息。

这两种方式的改进在有些复杂的下游任务中是可以有较好的效果的，但是对于下游任务`AVE`的跨模态事件分类中效果是不如`CMG`模型优秀的，所以说对于具体的下游任务对模型进行具体的选择是有必要的

![image-20241213140119950](..\images\image-20241213140119950.png)

***

交流反馈：

- 继续深入的研究一下这个模型中具体模块的代码，深入了解其原理
- 也可以去了解一下`prototypical network`这个知识点



## 2025/01/13 第六次交流

之前是使用三个模态（视频，电流和文本模态）进行预训练，但是文本模态在预训练的时候好像暴露标签了，可能会对下游任务有影响，后面的预训练就直接把文本模态拿掉了，只是用了电流模态和视频模态进行预训练，让电流模态学习视频模态的细粒度表征，得到一个离散化的共享码本。

在下游任务中，在恢复预训练模型的基础上，先使用视频模态进行训练解码器分类头，再使用电流模态进行验证，通过二分类任务的多项指标来验证电流模态学习视频模态细粒度表征的程度。

在原先模型代码的基础上，我修改了一部分，原先模型是为了满足多个下游任务进行设计的，在预训练的时候，各种模态互相学习表征的程度和码本更新受各种模态的影响都是一致的，但是考虑到我们的数据中电流模态的噪声比较大，视频模态应该减少甚至不要去学习电流模态的细粒度表征，更应该关注电流模态去学习视频模态的细粒度表征即可。

对于`cross-cpc`模块，我尝试了不同的时间预测步，发现在预测为5步的时候效果相对其他较好一点；对于码字的大小，尝试了使用100个码字和400个码字，感觉区别不大，400个码字的情况稍微好一点

预训练在epoch为30的时候收敛，但是在第16个epoch的时候，下游任务电流模态的验证准确率是最好的，可能的原因是电流的噪声比较大，越往后训练，可能学到了不必要的噪声。虽然越往后训练，下游任务中视频模态训练的准确率会越来越好，可以达到95%以上，但是考虑到电流模态学习视频模态的细粒度表征这个问题，还是要以下游任务中电流模态二分类的指标为主。

在没有使用预训练模型下的下游任务日志情况：

![image-20250113182052894](..\images\image-20250113182052894.png)

> 即使视频模态在训练的过程中准确率慢慢上升了，但是电流模态指标的准确率是非常低的，基本可以理解为是一个随机预测，电流没有受到任何视频模态的影响。

在使用预训练模型下的下游任务日志情况：

![image-20250113182211365](..\images\image-20250113182211365.png)

> 视频模态在训练的过程中准确率慢慢上升了，电流模态指标的准确率也相对没有使用预训练模型上升了，达到了69%左右，说明共享的离散码本可以使电流模态学习到视频模态的一些表征。但是相较实际任务还是有点低，后续还要进行继续的优化。

我认为电流模态没有很好的学习到视频模态的细粒度表征，可能的问题出在下面的几个方面：

- 在不同模态映射到离散化码本的时候，往往是被映射到码本的不同区域，即映射到不同的码字上了，主要原因应该是不同的模态即使表现的都是异常的工况，但是毕竟是两个不同的模态，所体现的语义信息是不能完全一样的，映射到不同的码字上是一定会发生的，我也尝试过一些方法进行优化：

  - 在使用EMA更新码本时，让电流模态自身更新码本的权重减少，视频模态更新码本的权重变大；视频模态更新码本尽量少受电流模态的影响，电流模态更新码本尽量多受视频模态的影响

    ![image-20250113190200631](..\images\image-20250113190200631.png)

    这样在预训练的时候，不同的模态会被映射到相同码字的趋势，`equal_num`表示视频模态和电流模态有相同码字的情况（在预训练的时候设置的`Batch Size`为8，所以`equal_num`不会超过8个）

    ![image-20250113190447698](..\images\image-20250113190447698.png)

    但是这种情况下，相同的码字数量还是偏低，可能的原因是语义特征提取的不够好，后续可以从提取电流语义特征方面进行优化

  - 还尝试过对电流的语义特征使用特征融合或者跨模态学习视频语义的一些信息，想让后续电流模态映射到具体码字时，可以更向视频模态映射的码字靠近

    ![image-20250113191259059](..\images\image-20250113191259059.png)

    > 可以看到`equal_num`共同码字的个数是比较大的，基本上接近于`Batch Size`的个数了，而且这种情况`acc_cv`指标的准确率上升的也很快，同时`cpc_loss`损失下降的也非常快
    >
    > 但是这种情况不适合下游任务，下游任务我们只使用电流的模态进行验证，如果融合或者使用跨模态的方法就失去了意义。

  - 还有个原因可能是，码字初始化的范围的设置可能也是一个问题，码字的初始化范围和码字的数量应该需要进行调整（码字数量/范围太大，会导致特征被离散化到不同的码字上，无法跳出局部最优解；码字的数量/范围太小，可能无法有效的提取多样化的特征）

  

## 2025/02/19 第七次交流

与吴老师交流的论文的创新点，基于目前的模型，可以进行哪些方面的修改，凝练了以下合适的创新点：

- 结合具体的电熔镁炉的工业场景，突出电流模态的前瞻性作用：

  由于时序电流模态和视频模态对于异常工况发生的时序表现是不同的，电流模态往往是相较于视频模态提前一段时间发生的，如视频模态在t时刻发生异常工况，但是电流模态的异常可能发生在t-n到t-m（n>m>0）的时间段内，结合实际的应用场景和异常机理，来突出电流模态的在这个异常检测过程中的作用（视频异常发生的前瞻性作用，虽然电流模态的异常检测效率不高，但对异常发生的情况也有一部分指导意义，如果电流模态可以提前的较为准确的检测到异常的发生，可以及时的警示视频模态可能将要发生异常），电流模态是异常发生因，视频模态是异常发生的果（电流异常一段时间后，才会将异常反应到熔炉表面，被视频模态检测到），因此，能够提前的预测电流模态的异常发生，对视频模态进行异常检测有着提前的告警预知作用

- 结合Specific Encoder编码器进行考虑：

  探究该编码器是否具有冗余特征的分离作用，去掉或修改这一个编码器是否影响下游任务的性能；是否该编码器可以有效的对电流的模态进行去噪处理，来隔绝一些不必要的噪声，在对电流模态的噪声处理上进行优化

- 在时序方面进行研究，挖掘模态中更深的时序信息：

  码本在构建的过程中，时序方面是否能够进行进一步的体现；不同模态的特征在时序方面是否可以进行进一步的挖掘

对于上面的三个方面，我也觉得结合具体的电熔镁炉的工业场景进行展开会更能体现实际的意义，在这个场景的基础上进行展开，引出电流模态去学习视频模态细粒度表征的重要性，从而进行后续的跨模态对齐，电流模态学习视频模态的细粒度表征之后，可以较为有效的进行异常的检测（虽然的检测准确率可能不是很高，但是还是可以为后续的视频模态检测到异常工况的发生提供一些警示），这个逻辑是比较合理的

第二个创新点是从损失方面修改，通过优化损失的方式进行创新点的引入



## 2025/03/07 第八次交流

代码整体是使用预训练的方式将电流模态和视频模态进行细粒度级别的特征对齐，让电流模态学习视频模态的细粒度表征，采用互信息最小化，分离各个模态的未知语义特征和剩余特有特征，采用互信息最大化，来拉近各个不同模态的语义特征，一致化跨模态的一致性表征

在预训练的时候，对离散化的特征进行`T-SNE`可视化：

在训练开始的时候，可以看出不同模态的离散化特征映射到二维空间中，是没有过多的一致性的：

<img src="..\images\image-20250307164207942.png" alt="image-20250307164207942" style="zoom: 50%;" />

但是随着预训练的进行，模态间互相学习一致性表征，拉近语义一致的离散表示：

<img src="..\images\image-20250307164400161.png" alt="image-20250307164400161" style="zoom:50%;" />

如何进行损失的优化或者引入新的损失，来提升模型的性能：

- 首先思考的是使用三元损失`triplet_loss`进行优化，尝试是否可以辅助互信息的最小化和最大化，让不同模态的语义特征一个作为基准，一个作为正样本，模态各自的剩余特征作为负样本，拉近正样本（不同模态语义特征）的距离，推远负样本（同一模态语义特征和剩余专有特征）之间的距离，但是基于这个思路，在训练的过程中，发现`triplet_loss`在快速的下降，很快就收敛到0，这个情况显然是不太合理的，尝试修改了学习率和三元损失中的`margin`参数，影响不大，还是会快速的收敛，前面几个`epoch`训练出来的预训练空间下游任务的效果还行，较之前的模型有一点的提升，但是训练到后面的epoch，下游任务的效果就变得很差了，感觉是过多的约束和强制的干预这些特征导致模型出现了问题

- 其次，考虑的是对`CPC`模块中的损失进行优化，`CPC`的目标就是要做无监督表示学习，并且我们希望这个表征空间有很好的预测的能力，原先在`Cross_CPC`中使用的是`InfoNCE`损失来实现互信息的最大化，计算了不同的对比损失项，每个对比损失项都是通过矩阵乘法得到的，比如`total1`是`video_encode_samples[i]`（视频的真实未来嵌入）和`current_pred[i]`（电流的预测嵌入）的转置相乘。接下来，每个`total`经过`softmax`和`log softmax`处理，然后取对角线元素的和，乘以相应的权重（`w1`, `w3`,` w4`），累加到`nce`变量中，最后取平均得到最终的损失

  我就想是否可以对这个损失进行优化，我最近看到了一个`CWCL`损失（连续加权对比损）函数，该损失可以显著提升模型在跨模态对齐任务中的性能，尤其在数据噪声较多或样本相似性差异大的场景下

  ![image-20250307155132805](..\images\image-20250307155132805.png)

  损失函数（将` pi`与其他模态中的` qj`对齐）定义为:

![image-20250307155156657](..\images\image-20250307155156657.png)

​		这个损失论文中没有提供代码，现在还在尝试融合到我的模型中看效果

对于电流时间的前瞻性的问题，我初步考虑是通过判断预训练过程中的指标进行最佳提前时间的确定，设计的指标有：

- `c_perplexity`和`v_perplexity`

  码本中码字的使用频率的困惑度（`Perplexity`），困惑度越低，表示概率分布越集中，表示码本中的码字使用频率越集中，某些码字被频繁使用，而其他码字很少被使用

- `equal_num`

  一个批次中不同模态数据映射到具体码字是同一个码字的数量，`equal_num/batch_size`越大，说明映射的对齐程度越好

通过不断的调整提前的时间，根据指标的好坏，来确定最好的提前时间

还有一个问题需要和吴老师同步一下，下游任务使用的数据需要在预训练中出现（也就是说预训练的数据需要包含下游任务的这部分数据）才能有不错的效果，如果下游任务使用的数据没有在预训练模态中训练，效果就非常的差

虽然，我觉得预训练中的数据包括下游任务的数据应该也没有什么问题，预训练重点考虑的任务是模态间的细粒度一致性对齐任务，只是单单考虑不同模态的内部关系，不会暴露具体的异常/正常标签，而下游任务的重点是异常检测二分类的任务，使用到具体的标签进行异常的检测。但是这样的效果是不是就失去的一定的泛化性能，如果使用新的数据进行下游任务，都需要先拿这个数据先进行预训练，这是不是不太合理（就是想和老师同步一下是不是可以忽略掉泛化这个部分，只考虑不同模态的细粒度语义一致性的对齐效果）
